{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praca Domowa - Modelowanie Wieloagentowe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podane są 8 poleceń, w ramach obowiązkowej pracy domowej należy wybrać i wykonać dwa z nich. Pozostałe można traktować jako dodatkowe zadania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "W zaprezentowanym na ćwiczeniach środowisku <tt>Frozen Lake</tt> zaimplementuj algorytm Monte Carlo Off Policy Learning.\n",
    "\n",
    "## Zadanie 2\n",
    "\n",
    "1. Zmodyfikuj prezentowane na zajęciach kody do algorytmów $SARSA$ i $Q-learning$ tak aby paramert eksploracji $\\epsilon$ malał liniowo z czasem.\n",
    "2. W tak zmodyfikowanym przykładzie postataraj się tak dobrać parametry (stopa uczenia $\\alpha$, długość procesu eksploracji (<i>wypalania</i>)) aby algorytm $SARSA$ zbiegł do rozwiązania optymalnego.\n",
    "\n",
    "## Zadanie 3\n",
    "\n",
    "W środowisku <tt>Frozen Lake</tt> zaimplementuj algorytm $SARSA(\\lambda)$ ze śladami wybieralności (<i>eligibility  traces</i>). Porównaj jego efektywność z  bazowym algorytmem $SARSA(0)$ dla dla map o rozmiarze $n \\in [10,100,200,\\dots,1000]$.\n",
    "\n",
    "## Zadanie 4\n",
    "\n",
    "W prezentowanym na zajęciach przykładzie działania algorytmu $DynaQ$:\n",
    "\n",
    "1. Zmodyfikuj estymator modelu tak, aby uwzględniał stochastyczność otoczenia. \n",
    "2. Zaimplementuj algorytm $DynaQ+$. Porównaj oba algorytmy dla stochastycznej wersji środowiska <tt>Frozen Lake</tt>.\n",
    "\n",
    "## Zadanie 5\n",
    "\n",
    "Zmodyfikuj prezentowane rozwiązanie problemu <tt>Mountain Car</tt> tak aby otrzymywana przez agenta przeciętna nagroda na koniec epizodu było możliwie jak najwyższa. Przyjmij, że uczenie trwa przez 10 000 epizodów.\n",
    "\n",
    "<b>Uwaga: Nie modyfikuj nagród otrzymywanych przez agenta!</b>\n",
    "\n",
    "## Zadanie 6\n",
    "\n",
    "Zaimplementuj algorytm REINFORCE i algorytm REINFORCE z funkcją bazową w dowolnym prezentowanym na zajęciach środowisku. Przyjmij, że $b(s) = \\hat v(s,\\theta)$. Porównaj wyniki działania obu algorytmów.\n",
    "\n",
    "## Zadanie 7\n",
    "\n",
    "Zaimplementuj algorytm aktora-krytyka w ciągłej wersji środowiska <tt>Mountain Car</tt>.\n",
    "\n",
    "## Zadanie 8\n",
    "\n",
    "Podczas zajęć pokazalśmy, że algorytmy on-policy (takie jak metoda aktora-krytyka) nie radzą sobie z problemami, w których mechanizm nagradzania jest rzadki, czyli takich w których agent dostaje nagrody np. jedynie na koniec epizodu. \n",
    "\n",
    "Problem <tt>Mountain Car</tt> jest jednym z takich problemów. Agent dostaje nagrodę jedynie gdy dojedzie na szczyt wzgórza po prawo, dopóki tego nie dokona nie jest w stanie efektywnie poprawiać swojej strategii. Jedną z metod rozwiązania tego problemu jest modyfikacja mechanizmu nagradzania w taki sposób żeby agent był nagradzany znacznie częściej.\n",
    "\n",
    "Celem zadania jest wprowadzenie takiej zmiany nagród aby agent korzystający z metody aktora-krytyka był w stanie nauczyć się rozwiązywać tego problemu. Może ona wymagać modyfikacji sposobu działania środowiska, kod dostępny jest [tutaj](https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/classic_control/mountain_car.jl). \n",
    "\n",
    "Zadanie jest warte 20 punktów, poza poprawności kodu sprawdzane będą 2 rzeczy: złożoność modyfikacji (im mniejsza jest zmiana reguł tym lepiej) i czas nauczenia konieczny do nauczenia się optymalnego rozwiązania."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
